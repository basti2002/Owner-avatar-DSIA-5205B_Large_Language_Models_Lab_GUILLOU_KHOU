{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6c5f27",
   "metadata": {},
   "source": [
    "# Bastien GUILLOU et Ryan KHOU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 5 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "# Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860ba9f-2db3-4480-b96b-4be1c68981eb",
   "metadata": {},
   "source": [
    "**Empirical Analysis of Token Sampling Frequencies Under Temperature Scaling**\n",
    "\n",
    "**Key Research Question: How does temperature-based scaling of the `softmax` probability distribution impact the sampling frequency of the specific lexical token `\"pizza\"`?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Utilize the `print_sampled_tokens` function to:\n",
    "- Empirically examine token sampling probabilities\n",
    "- Analyze the impact of temperature scaling\n",
    "- Quantify the sampling occurrence of the `\"pizza\"` token\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Determine the precise sampling frequency of `\"pizza\"` across different temperature configurations\n",
    "- Critically evaluate the current computational approach to sampling frequency measurement\n",
    "- Explore potential methodological improvements for more efficient and accurate token sampling analysis\n",
    "\n",
    "*Key Investigative Parameters:*\n",
    "- Primary token of interest: `\"pizza\"`\n",
    "- Sampling method: Temperature-scaled `softmax` distribution\n",
    "- Computational tool: `print_sampled_tokens` function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510ffb0-adca-4d64-8a12-38c4646fd736",
   "metadata": {},
   "source": [
    "# Exercise 5.2: Different temperature and top-k settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884990db-d1a6-4c4e-8e36-2c1e4c1e67c7",
   "metadata": {},
   "source": [
    "**Empirical Investigation of Generative Language Model Sampling Parameters**\n",
    "\n",
    "**Key Research Question: How do variations in `temperature` and `top-k` sampling parameters influence the qualitative and probabilistic characteristics of token generation in stochastic language models?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a systematic empirical exploration of:\n",
    "- Temperature scaling dynamics\n",
    "- Top-k probability truncation mechanisms\n",
    "- Generative output characteristics across different parameter configurations\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Identify contextual applications that benefit from lower `temperature` and `top-k` settings\n",
    "- Explore potential use cases preferring higher `temperature` and `top-k` configurations\n",
    "- Develop nuanced understanding of sampling parameter impact on generative outputs\n",
    "\n",
    "*Investigative Dimensions:*\n",
    "1. Low `temperature` and `top-k` Scenarios\n",
    "   - Potential applications\n",
    "   - Characteristics of generated outputs\n",
    "   - Contextual relevance\n",
    "\n",
    "2. High `temperature` and `top-k` Scenarios\n",
    "   - Potential applications\n",
    "   - Characteristics of generated outputs\n",
    "   - Contextual relevance\n",
    "\n",
    "*Recommended Experimental Protocol:*\n",
    "1. Systematically vary `temperature` and `top-k` parameters\n",
    "2. Meticulously document generative output characteristics\n",
    "3. Critically analyze observed variations\n",
    "4. Develop hypotheses about optimal parameter configurations for specific applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35425d-529d-4179-a1c4-63cb8b25b156",
   "metadata": {},
   "source": [
    "# Exercise 5.3: Deterministic behavior in the decoding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12229a2-1d52-46ff-b1e8-198f2e58a7d2",
   "metadata": {},
   "source": [
    "**Deterministic Token Generation: Parametric Strategies for Eliminating Stochastic Variability**\n",
    "\n",
    "**Key Research Question: What specific configuration parameters within the `generate` function can systematically eliminate randomness to ensure consistently reproducible generative outputs?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "*Investigate comprehensive strategies to:*\n",
    "- Suppress stochastic token generation mechanisms\n",
    "- Enforce deterministic computational behavior\n",
    "- Replicate the predictable output characteristics of `generate_simple`\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Identify all potential parameter combinations\n",
    "- Systematically neutralize probabilistic sampling variations\n",
    "- Establish deterministic generative protocol\n",
    "\n",
    "*Critical Configuration Parameters to Examine:*\n",
    "1. `temperature` scaling\n",
    "2. `top_k` pruning mechanism\n",
    "3. Random seed initialization\n",
    "4. Sampling strategy selection\n",
    "\n",
    "*Recommended Experimental Protocol:*\n",
    "1. Analyze individual parameter impacts\n",
    "2. Identify minimal configuration requirements\n",
    "3. Validate deterministic output generation\n",
    "4. Compare against `generate_simple` implementation\n",
    "\n",
    "*Computational Implications:*\n",
    "- Understanding stochastic suppression mechanisms\n",
    "- Insights into generative model controllability\n",
    "- Strategies for reproducible machine learning outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0480e5-fb4e-41f8-a161-7ac980d71d47",
   "metadata": {},
   "source": [
    "# Exercise 5.4: Continued pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40044e8-a0f5-476c-99fd-489b999fd80a",
   "metadata": {},
   "source": [
    "**Continuation of Model Training: Stateful Resumption and Persistent Learning Dynamics**\n",
    "\n",
    "**Key Research Question: How can we effectively restore a machine learning model's training state across separate computational sessions, enabling seamless continuation of the pretraining process?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Implement a comprehensive model and optimizer state restoration strategy involving:\n",
    "- Weight reconstruction\n",
    "- Optimizer state recovery\n",
    "- Resumption of training from previously interrupted state\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Demonstrate stateful model persistence\n",
    "- Execute additional training epoch using restored model configuration\n",
    "- Validate continuity of learning progression\n",
    "\n",
    "*Critical Procedural Steps:*\n",
    "1. Load previously saved model weights\n",
    "2. Reconstruct optimizer internal state\n",
    "3. Reinitiate training using `train_model_simple` function\n",
    "4. Complete one additional training epoch\n",
    "\n",
    "*Recommended Implementation Strategy:*\n",
    "- Utilize precise weight and optimizer state loading mechanisms\n",
    "- Verify complete state restoration\n",
    "- Execute uninterrupted additional training epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384e788-f5a1-407c-8dd1-87959b75026d",
   "metadata": {},
   "source": [
    "# Exercise 5.5: Training and validation set losses of the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1140b-2027-4156-8d19-600ac849edbe",
   "metadata": {},
   "source": [
    "**Comparative Loss Assessment: Pretrained Model Performance on Specialized Textual Domain**\n",
    "\n",
    "**Key Research Question: What are the comparative training and validation set losses when applying a pretrained OpenAI `GPTModel` to the \"The Verdict\" dataset?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a comprehensive loss evaluation involving:\n",
    "- Model weight initialization from pretrained OpenAI configuration\n",
    "- Computational loss calculation across training and validation datasets\n",
    "- Quantitative performance assessment in domain-specific context\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Determine precise loss metrics for training dataset\n",
    "- Calculate validation set loss\n",
    "- Interpret performance characteristics of pretrained model on specialized textual domain\n",
    "\n",
    "*Critical Computational Procedures:*\n",
    "1. Load pretrained OpenAI `GPTModel` weights\n",
    "2. Prepare \"The Verdict\" dataset\n",
    "3. Compute training set loss\n",
    "4. Compute validation set loss\n",
    "5. Comparative loss analysis\n",
    "\n",
    "*Investigative Parameters:*\n",
    "- Model: Pretrained OpenAI `GPTModel`\n",
    "- Dataset: \"The Verdict\"\n",
    "- Metrics: Training and validation loss measurements\n",
    "\n",
    "*Recommended Analytical Approach:*\n",
    "- Implement precise loss computation\n",
    "- Validate computational methodology\n",
    "- Critically interpret loss metric implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76a1e0-9635-480a-9391-3bda7aea402d",
   "metadata": {},
   "source": [
    "# Exercise 5.6: Trying larger models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d313f4-0038-4bc9-a340-84b3b55dc0e3",
   "metadata": {},
   "source": [
    "**Comparative Generative Analysis: Scale and Performance Variations in GPT-2 Model Architectures**\n",
    "\n",
    "**Key Research Question: How do generative text characteristics vary across different GPT-2 model scales, specifically comparing the 124 million and 1,558 million parameter configurations?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a systematic comparative investigation of:\n",
    "- Generative text quality\n",
    "- Semantic coherence\n",
    "- Linguistic complexity\n",
    "- Contextual understanding\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Empirically assess generative performance across model scales\n",
    "- Identify qualitative differences in text generation\n",
    "- Explore the relationship between model parameter count and generative capabilities\n",
    "\n",
    "*Comparative Model Configurations:*\n",
    "1. Smaller Model: **124 million parameters**\n",
    "2. Larger Model: **1,558 million parameters**\n",
    "\n",
    "*Investigative Dimensions:*\n",
    "- Textual coherence\n",
    "- Semantic precision\n",
    "- Contextual relevance\n",
    "- Linguistic nuance\n",
    "- Complexity of generated content\n",
    "\n",
    "*Experimental Protocol:*\n",
    "1. Generate text samples using both model configurations\n",
    "2. Conduct qualitative comparative analysis\n",
    "3. Assess generative performance across multiple dimensions\n",
    "4. Document observable variations in text generation characteristics\n",
    "\n",
    "*Recommended Analytical Approach:*\n",
    "- Utilize consistent generation parameters\n",
    "- Employ multiple generation trials\n",
    "- Implement rigorous qualitative assessment\n",
    "- Develop comprehensive comparative framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e22eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf877f",
   "metadata": {},
   "source": [
    "# Answers\n",
    "\n",
    "## Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46b4570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from torch) (69.0.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\raize\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406d2215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Temperature : 1.0 ===\n",
      "64 x 'closer'\n",
      "3 x 'every'\n",
      "0 x 'effort'\n",
      "572 x 'forward'\n",
      "2 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "356 x 'toward'\n",
      "3 x 'you'\n",
      "\n",
      "=== Temperature : 0.1 ===\n",
      "0 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "990 x 'forward'\n",
      "0 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "10 x 'toward'\n",
      "0 x 'you'\n",
      "\n",
      "=== Temperature : 5.0 ===\n",
      "157 x 'closer'\n",
      "70 x 'every'\n",
      "46 x 'effort'\n",
      "261 x 'forward'\n",
      "79 x 'inches'\n",
      "39 x 'moves'\n",
      "41 x 'pizza'\n",
      "210 x 'toward'\n",
      "97 x 'you'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "\n",
    "def print_sampled_tokens(probas, num_samples=1000):\n",
    "    torch.manual_seed(123)  \n",
    "    sampled_tokens = torch.multinomial(probas, num_samples=num_samples, replacement=True)\n",
    "    sampled_counts = torch.bincount(sampled_tokens, minlength=len(vocab))\n",
    "\n",
    "    for idx, count in enumerate(sampled_counts):\n",
    "        token = inverse_vocab.get(idx, \"[UNK]\")\n",
    "        print(f\"{count} x '{token}'\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "temperature_values = [1.0, 0.1, 5.0]\n",
    "\n",
    "for temp in temperature_values:\n",
    "    print(f\"\\n=== Temperature : {temp} ===\")\n",
    "    probabilities = softmax_with_temperature(next_token_logits, temp)\n",
    "    print_sampled_tokens(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9f763",
   "metadata": {},
   "source": [
    "## Exercise 5.2: Different temperature and top-k settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca4c2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Temperature: 0.5, Top-k: 3 ===\n",
      "7 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "714 x 'forward'\n",
      "0 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "279 x 'toward'\n",
      "0 x 'you'\n",
      "\n",
      "=== Temperature: 0.5, Top-k: 5 ===\n",
      "7 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "714 x 'forward'\n",
      "0 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "279 x 'toward'\n",
      "0 x 'you'\n",
      "\n",
      "=== Temperature: 0.5, Top-k: 9 ===\n",
      "7 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "714 x 'forward'\n",
      "0 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "279 x 'toward'\n",
      "0 x 'you'\n",
      "\n",
      "=== Temperature: 1.0, Top-k: 3 ===\n",
      "65 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "576 x 'forward'\n",
      "0 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "359 x 'toward'\n",
      "0 x 'you'\n",
      "\n",
      "=== Temperature: 1.0, Top-k: 5 ===\n",
      "64 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "575 x 'forward'\n",
      "2 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "356 x 'toward'\n",
      "3 x 'you'\n",
      "\n",
      "=== Temperature: 1.0, Top-k: 9 ===\n",
      "64 x 'closer'\n",
      "3 x 'every'\n",
      "0 x 'effort'\n",
      "572 x 'forward'\n",
      "2 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "356 x 'toward'\n",
      "3 x 'you'\n",
      "\n",
      "=== Temperature: 2.0, Top-k: 3 ===\n",
      "156 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "473 x 'forward'\n",
      "0 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "371 x 'toward'\n",
      "0 x 'you'\n",
      "\n",
      "=== Temperature: 2.0, Top-k: 5 ===\n",
      "150 x 'closer'\n",
      "0 x 'every'\n",
      "0 x 'effort'\n",
      "450 x 'forward'\n",
      "24 x 'inches'\n",
      "0 x 'moves'\n",
      "0 x 'pizza'\n",
      "332 x 'toward'\n",
      "44 x 'you'\n",
      "\n",
      "=== Temperature: 2.0, Top-k: 9 ===\n",
      "144 x 'closer'\n",
      "20 x 'every'\n",
      "3 x 'effort'\n",
      "437 x 'forward'\n",
      "22 x 'inches'\n",
      "3 x 'moves'\n",
      "8 x 'pizza'\n",
      "321 x 'toward'\n",
      "42 x 'you'\n"
     ]
    }
   ],
   "source": [
    "# Function to apply top-k truncation\n",
    "def apply_top_k_truncation(probas, k):\n",
    "    \"\"\"\n",
    "    Truncate the probabilities to keep only the top-k values.\n",
    "    \"\"\"\n",
    "    if k >= len(probas):\n",
    "        return probas  # No truncation if k is equal to or larger than the vocabulary size\n",
    "    topk_values, topk_indices = torch.topk(probas, k)\n",
    "    truncated_probas = torch.zeros_like(probas)\n",
    "    truncated_probas[topk_indices] = topk_values\n",
    "    return truncated_probas / truncated_probas.sum()\n",
    "\n",
    "# Function to explore sampling parameters\n",
    "def explore_sampling_parameters(logits, temperatures, top_ks, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Explore sampling variations by temperature and top-k values.\n",
    "    \"\"\"\n",
    "    for temp in temperatures:\n",
    "        for k in top_ks:\n",
    "            print(f\"\\n=== Temperature: {temp}, Top-k: {k} ===\")\n",
    "            # Apply temperature scaling\n",
    "            probabilities = softmax_with_temperature(logits, temp)\n",
    "            # Apply top-k truncation\n",
    "            truncated_probabilities = apply_top_k_truncation(probabilities, k)\n",
    "            # Sample and display frequencies\n",
    "            print_sampled_tokens(truncated_probabilities, num_samples)\n",
    "\n",
    "# Parameters for the experiment\n",
    "temperature_values = [0.5, 1.0, 2.0]\n",
    "top_k_values = [3, 5, len(vocab)]\n",
    "\n",
    "# Execute the exploration\n",
    "explore_sampling_parameters(next_token_logits, temperature_values, top_k_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6f71c",
   "metadata": {},
   "source": [
    "## Exercise 5.3: Deterministic behavior in the decoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29924a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text,load_weights_into_gpt\n",
    "import tiktoken\n",
    "\n",
    "import torch\n",
    "from previous_labs import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval();  # Disable dropout during inference\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62e918",
   "metadata": {},
   "source": [
    "# Exercise 5.4: Continued pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae49fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.369, Val loss 6.503\n",
      "Ep 1 (Step 000005): Train loss 0.338, Val loss 6.515\n",
      "Every effort moves you?\" \"I and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\" \"I looked at the donkey again. I saw that, when Stroud laid in the first\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_labs import create_dataloader_v1\n",
    "from gpt_train import train_model_simple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://huggingface.co/datasets/DarwinAnim8or/the-verdict/blob/main/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "        \n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c460819",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23951ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();\n",
    "\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
