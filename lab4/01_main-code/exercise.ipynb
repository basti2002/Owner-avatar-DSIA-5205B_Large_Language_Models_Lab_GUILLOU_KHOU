{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e831f0c6",
   "metadata": {},
   "source": [
    "# Bastien GUILLOU et Ryan KHOU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 4 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "# Exercise 4.1: Parameters in the feed forward versus attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d2157e",
   "metadata": {},
   "source": [
    "**Key Exercise Question: How do the parameter counts differ between the `feed-forward` neural network module and `multi-head attention` mechanism in our transformer architecture?**\n",
    "\n",
    "*Methodological Approach:*\n",
    "The investigation focuses on a systematic computational analysis of parameter allocation across two critical transformer neural network components:\n",
    "\n",
    "1. **Feed-Forward Neural Network Module**\n",
    "   - Characterization: Nonlinear transformation module\n",
    "   - Primary computational function: Introducing network complexity and representational capacity\n",
    "   - Parametric considerations: Linear transformation layers, activation functions\n",
    "\n",
    "2. **Multi-Head Attention Mechanism**\n",
    "   - Characterization: Contextual feature interaction module\n",
    "   - Primary computational function: Capturing inter-token relational dynamics\n",
    "   - Parametric considerations: Projection matrices, attention computation\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Quantify the exact number of trainable parameters in each architectural component\n",
    "- Comparative assessment of parametric complexity\n",
    "- Understand the relative computational resource allocation\n",
    "\n",
    "*Theoretical Implications:*\n",
    "- Insights into architectural parameter efficiency\n",
    "- Empirical understanding of transformer module design\n",
    "- Potential implications for model optimization and architectural design\n",
    "\n",
    "*Computational Methodology:*\n",
    "1. Enumerate parameters in `feed-forward` module\n",
    "2. Enumerate parameters in `multi-head attention` module\n",
    "3. Perform comparative statistical analysis\n",
    "4. Interpret parametric distribution characteristics\n",
    "\n",
    "*Recommended Investigative Approach:*\n",
    "- Utilize precise computational tracing\n",
    "- Consider layer-specific parameter counting\n",
    "- Account for bias terms and weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123ab2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Feed-Forward Module Parameters: 4722432\n",
      "Multi-Head Attention Parameters: 2360064\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gpt import TransformerBlock\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "print(block)\n",
    "\n",
    "# Count parameters\n",
    "params_ff = sum(p.numel() for p in block.ff.parameters() if p.requires_grad)\n",
    "params_mha = sum(p.numel() for p in block.att.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Feed-Forward Module Parameters: {params_ff}\")\n",
    "print(f\"Multi-Head Attention Parameters: {params_mha}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74ffdc",
   "metadata": {},
   "source": [
    "1. Enumerate parameters in feed-forward module\n",
    "\n",
    "The feed-forward module of the transformer block consists of two linear layers. The first transforms the entries of 768 units into 3072 units (intermediate activation), and a second reduces the dimensions from 3072 to 768. What we get:\n",
    "\n",
    "- First layer: 768 3072+3072 (bias) = 2,362,368\n",
    "- Second layer: 3072 768+768 = 2,360,064\n",
    "\n",
    "Total 4,722,432 parameters for the feed-forward module.\n",
    "\n",
    "2. Enumerate parameters in multi-head attention module\n",
    "\n",
    "The multi-head attention module has four linear matrices (for queries, keys, values and output projection) each with a 768x768 dimension of which three of these matrices have no bias terms and the fourth includes a bias:\n",
    "\n",
    "- Matrices for queries, keys, values: 3 (768 768) = 1,769,472\n",
    "- Output projection (with bias): 768 768+768 = 590.592 \n",
    "    \n",
    "That is a total of 2,360,064 parameters for multi-head attention.\n",
    "\n",
    "3. Perform comparative statistical analysis\n",
    "\n",
    "The feed-forward module contains almost double the parameters of the multi-head attention module, namely:\n",
    "\n",
    "- Feed-forward: 4,722,432 parameters\n",
    "- Multi-head attention: 2,360,368 parameters\n",
    "\n",
    "4. Interpret parametric distribution characteristics\n",
    "\n",
    "The feed-forward module uses more parameters to perform its linear transformations, which allows it to make the model more complex and able to handle various representations of data. This helps the model learn and adapt more effectively to different characteristics of input data.\n",
    "\n",
    "Whereas the multi-head attention module, which has fewer parameters, is designed to focus mainly on analyzing the relationships between different tokens in a sequence. This module does not introduce significant complexity to the model, but it is essential for interpreting how the various elements of an input are related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b7c7f-0fa1-4d30-ab44-e499edd55b6d",
   "metadata": {},
   "source": [
    "# Exercise 4.2: Initialize larger GPT models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b2e05-3ec8-47fc-afd9-83bf03d4aad8",
   "metadata": {},
   "source": [
    "- **GPT2-small** (the 124M configuration we already implemented):\n",
    "    - \"emb_dim\" = 768\n",
    "    - \"n_layers\" = 12\n",
    "    - \"n_heads\" = 12\n",
    "\n",
    "- **GPT2-medium:**\n",
    "    - \"emb_dim\" = 1024\n",
    "    - \"n_layers\" = 24\n",
    "    - \"n_heads\" = 16\n",
    "\n",
    "- **GPT2-large:**\n",
    "    - \"emb_dim\" = 1280\n",
    "    - \"n_layers\" = 36\n",
    "    - \"n_heads\" = 20\n",
    "\n",
    "- **GPT2-XL:**\n",
    "    - \"emb_dim\" = 1600\n",
    "    - \"n_layers\" = 48\n",
    "    - \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed1fd4",
   "metadata": {},
   "source": [
    "**Key Exercise Question: Can you systematically scale the GPT-2 model architecture from the small configuration to medium, large, and XL variants by exclusively modifying the configuration parameters?**\n",
    "\n",
    "*Architectural Scaling Challenge:*\n",
    "This exercise explores the methodological expansion of the GPT-2 model across different scales, demonstrating how architectural complexity can be incrementally increased through strategic parameter modifications.\n",
    "\n",
    "*Model Variants to Implement:*\n",
    "1. **GPT-2 Small (Current Implementation)**\n",
    "   - Embedding Dimensions (\"emb_dim\"): 768\n",
    "   - Transformer Blocks (\"n_layers\"): 12\n",
    "   - Multi-Head Attention Heads (\"n_heads\"): 12\n",
    "\n",
    "2. **GPT-2 Medium**\n",
    "   - Embedding Dimensions (\"emb_dim\"): 1,024\n",
    "   - Transformer Blocks (\"n_layers\"): 24\n",
    "   - Multi-Head Attention Heads (\"n_heads\"): 16\n",
    "\n",
    "3. **GPT-2 Large**\n",
    "   - Embedding Dimensions (\"emb_dim\"): 1,280\n",
    "   - Transformer Blocks (\"n_layers\"): 36\n",
    "   - Multi-Head Attention Heads (\"n_heads\"): 20\n",
    "\n",
    "4. **GPT-2 XL**\n",
    "   - Embedding Dimensions (\"emb_dim\"): 1,600\n",
    "   - Transformer Blocks (\"n_layers\"): 48\n",
    "   - Multi-Head Attention Heads (\"n_heads\"): 25\n",
    "\n",
    "*Methodological Constraints:*\n",
    "- Modify only the configuration file\n",
    "- Utilize the existing `GPTModel` class without code alterations\n",
    "- Demonstrate parameter scaling capabilities\n",
    "- Calculate total parameters for each model variant\n",
    "\n",
    "**Bonus Challenge:**\n",
    "**Compute the total number of trainable parameters for each model variant, highlighting the exponential growth in model complexity.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2ea54",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f793f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT-2 Small\n",
      "Total number of parameters: 163,009,536\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n",
      "\n",
      "\n",
      "Model: GPT-2 Medium\n",
      "Total number of parameters: 406,212,608\n",
      "Number of trainable parameters considering weight tying: 354,749,440\n",
      "\n",
      "\n",
      "Model: GPT-2 Large\n",
      "Total number of parameters: 838,220,800\n",
      "Number of trainable parameters considering weight tying: 773,891,840\n",
      "\n",
      "\n",
      "Model: GPT-2 XL\n",
      "Total number of parameters: 1,637,792,000\n",
      "Number of trainable parameters considering weight tying: 1,557,380,800\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gpt import GPTModel\n",
    "\n",
    "configurations = {\n",
    "    \"GPT-2 Small\": {\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_layers\": 12,\n",
    "        \"n_heads\": 12,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,  \n",
    "        \"drop_rate\": 0.1,  \n",
    "        \"qkv_bias\": False      \n",
    "    },\n",
    "    \"GPT-2 Medium\": {\n",
    "        \"emb_dim\": 1024,\n",
    "        \"n_layers\": 24,\n",
    "        \"n_heads\": 16,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False    \n",
    "    },\n",
    "    \"GPT-2 Large\": {\n",
    "        \"emb_dim\": 1280,\n",
    "        \"n_layers\": 36,\n",
    "        \"n_heads\": 20,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False      \n",
    "    },\n",
    "    \"GPT-2 XL\": {\n",
    "        \"emb_dim\": 1600,\n",
    "        \"n_layers\": 48,\n",
    "        \"n_heads\": 25,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False       \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "models = {}\n",
    "for name, cfg in configurations.items():\n",
    "    model = GPTModel(cfg)\n",
    "    models[name] = model\n",
    "    print(f\"Model: {name}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "    total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "    print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2306e-5dc8-498e-92ee-70ae7ec37ac1",
   "metadata": {},
   "source": [
    "# Exercise 4.3: Using separate dropout parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cc6cd",
   "metadata": {},
   "source": [
    "**Key Exercise Question: How can we enhance the dropout configuration of the GPT model by implementing layer-specific dropout rates?**\n",
    "\n",
    "*Architectural Dropout Refinement:*\n",
    "The current implementation employs a uniform dropout rate across multiple model components, which presents an opportunity for more nuanced regularization strategies. This exercise challenges you to develop a more sophisticated approach to dropout implementation within neural network architectures.\n",
    "\n",
    "*Dropout Localization:*\n",
    "Three critical architectural components require distinct dropout configurations:\n",
    "1. Embedding Layer\n",
    "2. Shortcut (Residual) Connections\n",
    "3. Multi-Head Attention Module\n",
    "\n",
    "*Methodological Approach:*\n",
    "You must modify the existing `GPT_CONFIG_124M` configuration to:\n",
    "- Replace the monolithic `drop_rate` parameter\n",
    "- Introduce a hierarchical dropout configuration\n",
    "- Maintain the overall structural integrity of the model architecture\n",
    "\n",
    "*Conceptual Challenge:*\n",
    "The exercise requires a deep understanding of:\n",
    "- Regularization techniques in neural network design\n",
    "- The functional role of dropout in different architectural components\n",
    "- Systematic configuration of model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f4c87",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e02c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  \n",
    "    \"context_length\": 1024, \n",
    "    \"emb_dim\": 768,          \n",
    "    \"n_heads\": 12,           \n",
    "    \"n_layers\": 12,         \n",
    "    \"drop_rate_emb\": 0.1,  \n",
    "    \"drop_rate_res\": 0.05,   \n",
    "    \"drop_rate_attn\": 0.15,  \n",
    "    \"qkv_bias\": False        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6465be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import LayerNorm, MultiHeadAttention, FeedForward\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"], \n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate_attn\"],  # Attention dropout rate\n",
    "            qkv_bias=cfg[\"qkv_bias\"] \n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate_res\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate_emb\"]) # Attention dropout rate\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5363c8",
   "metadata": {},
   "source": [
    "We can test our change like this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dd79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful. Output shape: torch.Size([1, 1024, 50257])\n"
     ]
    }
   ],
   "source": [
    "def test_model_initialization_and_forward_pass():\n",
    "    cfg = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_layers\": 12,\n",
    "        \"drop_rate_emb\": 0.1,\n",
    "        \"drop_rate_res\": 0.05,\n",
    "        \"drop_rate_attn\": 0.15,\n",
    "        \"qkv_bias\": False\n",
    "    }\n",
    "\n",
    "    model = GPTModel(cfg)\n",
    "    model.eval() \n",
    "    \n",
    "    dummy_input = torch.randint(0, cfg['vocab_size'], (1, cfg['context_length']))\n",
    "    \n",
    "    try:\n",
    "        output = model(dummy_input)\n",
    "        print(\"Forward pass successful. Output shape:\", output.shape)\n",
    "    except Exception as e:\n",
    "        print(\"Error during forward pass:\", e)\n",
    "\n",
    "test_model_initialization_and_forward_pass()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
